{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIOF050_AC:\n",
    "    \n",
    "\n",
    "    '''\n",
    "        \n",
    "        Inside this Net class, we can define what we want our convolutional autoencoder to look like!\n",
    "        \n",
    "    '''\n",
    "        \n",
    "    class AutoCNN(nn.Module):\n",
    "        \n",
    "        def __init__(self):\n",
    "            super(BIOF050_AC.AutoCNN, self).__init__()\n",
    "            \n",
    "            self.conv1 = nn.Conv2d(1, 16, 3)\n",
    "            self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "            \n",
    "            self.pool = nn.MaxPool2d(kernel_size=2,return_indices=True)\n",
    "            self.unpool = nn.MaxUnpool2d(2)\n",
    "            self.tanh = nn.Tanh()\n",
    "            self.relu = nn.ReLU()\n",
    "            self.unconv1 = nn.ConvTranspose2d(32, 16, 3)\n",
    "            self.unconv2 = nn.ConvTranspose2d(16, 1, 3)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \n",
    "            x = self.conv1(x)\n",
    "            x = self.relu(x)\n",
    "            x,indices1 = self.pool(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.relu(x)\n",
    "            x,indices2 = self.pool(x)\n",
    "\n",
    "            x = self.unpool(x,indices2,size2=(13,13))\n",
    "            x = self.unconv1(x)\n",
    "            x = self.relu(x)\n",
    "            \n",
    "            x = self.unpool(x,indices1,output_size=(26,26))\n",
    "            x = self.unconv2(x)\n",
    "            x = self.tanh(x)\n",
    "\n",
    "        \n",
    "            return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Now, we have to define the forward method, which takes a data point, or, in most cases, a batch, and\n",
    "        feeds it through all the layers of our neural network until assigning it a layer\n",
    "        \n",
    "        nn.Linear takes one array as an input, so we will input our data right into each layer, and then input the\n",
    "        outputs of each layer into the next layer\n",
    "        \n",
    "        After each layer, we will apply nn.ReLU to transform our data into a nonlinear space\n",
    "        \n",
    "        Finally, after the data has been passed through the output layer, we will convert it into a probaboility\n",
    "        distribution using the softmax function. \n",
    "        \n",
    "        This probabilty dsistribution will be used to assign a label to our\n",
    "        data points and to figure out just how well our neural network did, as we learned earlier today\n",
    "        \n",
    "        ''' \n",
    "        \n",
    "        def forward(self, batch):\n",
    "            batch = self.encoder(batch)\n",
    "            batch = self.decoder(batch)\n",
    "            return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our CNN\n",
    "Now, we will add in our train_test + batchify method from last time to use our CNN on a dataset! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIOF050_AC:\n",
    "    \n",
    "    \n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        \n",
    "        Inside this Net class, we can define what we want our convolutional neural network to look like!\n",
    "        We will define the convolutional layers AND the linear layers here \n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    class AutoCNN(nn.Module):\n",
    "        \n",
    "        def __init__(self):\n",
    "            \n",
    "            super(BIOF050_AC.AutoCNN, self).__init__()\n",
    "            ''' here, we define our convolution layers'''\n",
    "            self.conv1 = nn.Conv2d(1, 16, 3)\n",
    "            self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "            \n",
    "            ''' \n",
    "            max pooling - we need the indicies of the max vaues for unpooling\n",
    "            so return_indicies=True \n",
    "            '''\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2,return_indices=True)\n",
    "            \n",
    "            ''' now need an unpool,to remove the effects of pooling in the decoder '''\n",
    "            self.unpool = nn.MaxUnpool2d(2)\n",
    "            self.tanh = nn.Tanh()\n",
    "            self.relu = nn.ReLU()\n",
    "            \n",
    "            ''' Our transpose covolution layer'''\n",
    "            self.unconv1 = nn.ConvTranspose2d(32, 16, 3)\n",
    "            self.unconv2 = nn.ConvTranspose2d(16, 1, 3)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \n",
    "            x = self.conv1(x)\n",
    "            x = self.relu(x)\n",
    "            ''' pooling - indicies are returned '''\n",
    "            x,indices1 = self.pool(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.relu(x)\n",
    "            ''' pooling - indicies are returned'''\n",
    "            x,indices2 = self.pool(x)\n",
    "            \n",
    "            ''' how big is the unpooled image we want to recreate?\n",
    "            \n",
    "            well, we lose a pixel on each end when we do convolution because \n",
    "            the kernel has to fit onto the images, so the center pixel cannot be on the \n",
    "            edge unless we pad the images\n",
    "            \n",
    "            \n",
    "            We started with 28x28 -> 26x26 -> 13x13 (pooling) -> 11x11\n",
    "            \n",
    "            so we will need to unpool images of 11x11 and 26x26\n",
    "            '''\n",
    "            x = self.unpool(x,indices2,output_size=(11,11))\n",
    "            x = self.unconv1(x)\n",
    "            x = self.relu(x)\n",
    "            \n",
    "            x = self.unpool(x,indices1,output_size=(26,26))\n",
    "            x = self.unconv2(x)\n",
    "            x = self.tanh(x)\n",
    "\n",
    "        \n",
    "            return x\n",
    "\n",
    "\n",
    "    ''' We will not do any parameter optimization for this tutorial, so no need to have any\n",
    "    parameters for this method'''\n",
    "    def train_test(self):\n",
    "            \n",
    "           \n",
    "            batches = batchify_autoencoder(self.data,batch_size=16)\n",
    "  \n",
    "            neural_network = BIOF050_AC.AutoCNN()\n",
    "        \n",
    "            optimizer = optim.SGD(neural_network.parameters(), lr=0.01)\n",
    "        \n",
    "            loss_function = nn.MSELoss()\n",
    "        \n",
    "            neural_network.train()\n",
    "        \n",
    "            ### n_epochs\n",
    "            for i in range(3):\n",
    "                error = 0\n",
    "                for ii in range(len(batches)):\n",
    "                \n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                    batch = batches[ii]\n",
    "\n",
    "                    predictions = neural_network(torch.tensor(np.asarray(batch).astype(np.float32)))\n",
    "                    \n",
    "                    loss = loss_function(predictions,torch.tensor(np.asarray(batch).astype(np.float32)))\n",
    "                \n",
    "                    loss.backward()\n",
    "                \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    error += loss.data\n",
    "                    \n",
    "                print('Error: ' + str((error/len(self.data))*16))\n",
    "\n",
    "            return neural_network\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "''' Utility Function - function to turn the data into batches'''\n",
    "\n",
    "def batchify_autoencoder(data,batch_size=16):\n",
    "    \n",
    "    batches= []\n",
    "\n",
    "\n",
    "    for n in range(0,len(data),batch_size):\n",
    "        if n+batch_size < len(data):\n",
    "            batches.append(data[n:n+batch_size])\n",
    "            \n",
    "\n",
    "    if len(data)%batch_size > 0:\n",
    "        batches.append(data[len(data)-(len(data)%batch_size):len(data)])\n",
    "\n",
    "        \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torchvision.datasets.MNIST(\n",
    "    root = './data/MNIST',\n",
    "    download = True)\n",
    "\n",
    "\n",
    "labels = data.targets\n",
    "data = data.data\n",
    "newdata = []\n",
    "\n",
    "for image in data:\n",
    "   image = np.ravel(image).astype(np.float64)\n",
    "   image *= 1/image.max()\n",
    "   newdata.append(image.reshape(1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: tensor(0.0379)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-f826a2138b81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtestclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBIOF050_AC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-118-0a97431dd343>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-0a97431dd343>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m             '''\n\u001b[1;32m     64\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindices2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m         for hook in itertools.chain(\n\u001b[0m\u001b[1;32m    717\u001b[0m                 \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                 self._forward_pre_hooks.values()):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "testclass = BIOF050_AC(newdata)\n",
    "model = testclass.train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
