{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the update is only in the final class (BIOF050_CNN_Final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIOF050_CNN:\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        \n",
    "        Inside this Net class, we can define what we want our convolutional neural network to look like!\n",
    "        We will define the convolutional layers AND the linear layers here \n",
    "        \n",
    "        Inputs: \n",
    "        \n",
    "        the number of channels our images have (for black and white images,\n",
    "        this will be equal to 1, for colored images, this will usually be equal to 3)\n",
    "        \n",
    "        the hidden dimension of our linear layers (same as MLP - how many neurons do we want?)\n",
    "\n",
    "        the size of our 2D convolution kernel (input 3 = 3x3 kernel)\n",
    "\n",
    "        the number of unique labels in our dataset\n",
    "        \n",
    "        Remember, if something has weights that must be optimized (like a linear layer or a convolution layer), \n",
    "        it must go in the constructor (__init__)\n",
    "        \n",
    "        Otherwise, it can go in either forward or constructor (__init___)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    class CNet(nn.Module):\n",
    "        \n",
    "        def __init__(self,n_channels,hidden_dimension,kernel_size,n_classes,pool):\n",
    "\n",
    "            super(BIOF050_CNN.CNet, self).__init__()\n",
    "            \n",
    "        \n",
    "            ### first convolution layer - You need to know # of channels/inputs and number of kernels (outputs)\n",
    "            self.convolution1 = nn.Conv2d(n_channels,28,kernel_size=kernel_size)\n",
    "            \n",
    "            ###2-D max pooling\n",
    "            self.pool = nn.MaxPool2d(pool, pool)\n",
    "            \n",
    "            ### second convolution layer - first term is number of kernels from last conv layer,\n",
    "            ### second is number of outputs\n",
    "            self.convolution2 = nn.Conv2d(n_kernels,56,kernel_size=kernel_size)\n",
    "            \n",
    "            ### 2-D maxpooling\n",
    "            self.pool2 = nn.MaxPool2d(pool, pool)\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            MLP layer - the 1400 represents the output of the final maxpooling (MaxPool2d) \n",
    "            after being flattened - you will need to get this value for each dataset you use\n",
    "            '''\n",
    "            \n",
    "            self.layer1 = nn.Linear(1400,10)\n",
    "            \n",
    "            '''\n",
    "            this is the activation function - we will input our data into this function\n",
    "            and ReLU will be applied\n",
    "            \n",
    "            '''\n",
    "            self.relu = nn.ReLU()\n",
    "\n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "            \n",
    "    \n",
    "        '''\n",
    "        Now, we have to define the forward method, which takes a data point, or, in most cases, a batch, and\n",
    "        feeds it through all the layers of our neural network until assigning it a layer\n",
    "        \n",
    "        nn.Convolution and nn.Linear take one array/tensor as an input, so we will input our data right into each layer, and then input the\n",
    "        outputs of each layer into the next layer\n",
    "        \n",
    "        After each layer, we will apply nn.ReLU to transform our data into a nonlinear space\n",
    "        \n",
    "        Finally, after the data has been passed through the output layer, we will convert it into a probaboility\n",
    "        distribution using the softmax function. \n",
    "        \n",
    "        This probabilty dsistribution will be used to assign a label to our\n",
    "        data points and to figure out just how well our neural network did, as we learned earlier today\n",
    "        \n",
    "        ''' \n",
    "        \n",
    "        def forward(self, batch):\n",
    "            \n",
    "            \n",
    "            ### first convolution\n",
    "            batch = self.convolution1(batch)\n",
    "            \n",
    "            ## activation\n",
    "            batch = self.relu(batch)\n",
    "            \n",
    "            ### pooling\n",
    "            batch = self.pool(batch)\n",
    "            \n",
    "            ### second convolution\n",
    "            batch = self.convolution2(batch)\n",
    "            \n",
    "            ## activation\n",
    "            batch = self.relu(batch)\n",
    "            \n",
    "            batch = self.pool(batch)\n",
    "            \n",
    "            ## flatten out each image to be in 1D using this view function\n",
    "            batch = batch.view(batch.size(0), -1)\n",
    "            \n",
    "            ### MLP layer\n",
    "            batch = self.layer1(batch)\n",
    "            \n",
    "            ### probability dist.\n",
    "            return nn.functional.softmax(batch)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our CNN\n",
    "Now, we will add in our train_test + batchify method from last time to use our CNN on a dataset! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "we can use torchvision's dataset feature to download large datasets, such as MNIST (handwritten digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIOF050_CNN_Final:\n",
    "    \n",
    "    \n",
    "    def __init__(self,data,labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    '''\n",
    "        \n",
    "        Inside this CNet class, we can define what we want our convolutional neural network to look like!\n",
    "        We will define the convolutional layers AND the linear layers here \n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    \n",
    "    class CNet(nn.Module):\n",
    "        \n",
    "        def __init__(self,n_channels,hidden_dimension,kernel_size,n_classes,pool):\n",
    "\n",
    "            super(BIOF050_CNN_Final.CNet, self).__init__()\n",
    "\n",
    "            ### first convolution layer - You need to know # of channels/inputs and number of kernels (outputs)\n",
    "            self.convolution1 = nn.Conv2d(n_channels,28,kernel_size=kernel_size)\n",
    "            \n",
    "            ###2-D max pooling\n",
    "            self.pool = nn.MaxPool2d(pool, pool)\n",
    "            \n",
    "            ### second convolution layer - first term is number of kernels from last conv layer,\n",
    "            ### second is number of outputs\n",
    "            \n",
    "            self.convolution2 = nn.Conv2d(28,56,kernel_size=kernel_size)\n",
    "            \n",
    "            ### 2-D maxpooling\n",
    "            self.pool2 = nn.MaxPool2d(pool, pool)\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            MLP layer - the 3584 represents the output of the final maxpooling (MaxPool2d) \n",
    "            after being flattened - you will need to get this value for each dataset you use\n",
    "            '''\n",
    "            \n",
    "            self.layer1 = nn.Linear(find_MLP_dim(28,kernel_size,56),10)\n",
    "            self.relu = nn.ReLU()\n",
    "            \n",
    "\n",
    "            \n",
    "    \n",
    "        '''\n",
    "        Now, we have to define the forward method, which takes a data point, or, in most cases, a batch, and\n",
    "        feeds it through all the layers of our neural network until assigning it a layer\n",
    "        \n",
    "        nn.Convolution and nn.Linear take one array/tensor as an input, so we will input our data right into each layer, and then input the\n",
    "        outputs of each layer into the next layer\n",
    "        \n",
    "        After each layer, we will apply nn.ReLU to transform our data into a nonlinear space\n",
    "        \n",
    "        Finally, after the data has been passed through the output layer, we will convert it into a probaboility\n",
    "        distribution using the softmax function. \n",
    "        \n",
    "        This probabilty dsistribution will be used to assign a label to our\n",
    "        data points and to figure out just how well our neural network did, as we learned earlier today\n",
    "        \n",
    "        ''' \n",
    "        \n",
    "        def forward(self, batch):\n",
    "            \n",
    "            ### first convolution\n",
    "            batch = self.convolution1(batch)\n",
    "            \n",
    "            ## activation\n",
    "            batch = self.relu(batch)\n",
    "            \n",
    "            ## we could have batch norm if we wanted to \n",
    "            ## we could add in dropouts\n",
    "            \n",
    "            ### pooling\n",
    "            batch = self.pool(batch)\n",
    "            \n",
    "            ### second convolution\n",
    "            batch = self.convolution2(batch)\n",
    "            \n",
    "            ## activation\n",
    "            batch = self.relu(batch)\n",
    "            \n",
    "            batch = self.pool2(batch)\n",
    "            \n",
    "            ## flatten out each image to be in 1D using this view function\n",
    "            batch = batch.view(batch.size(0), -1)\n",
    "           \n",
    "            ### MLP layer\n",
    "            batch = self.layer1(batch)\n",
    "            \n",
    "            ### probability dist.\n",
    "            return nn.functional.softmax(batch)\n",
    "              \n",
    "       \n",
    "\n",
    "    \n",
    "           \n",
    "        \n",
    "    def train_test(self,test_size,n_epochs,n_channels,hidden_dimensions,batch_size,kernel_size,pool,lr):\n",
    "            \n",
    "        ### splitting the data into a training/testing set\n",
    "        train_data,test_data,train_labels,test_labels = train_test_split(self.data,self.labels, test_size=test_size)\n",
    "        \n",
    "        ## creating the batches using the batchify function\n",
    "        train_batches,train_label_batches = batchify(train_data,train_labels,batch_size=batch_size)\n",
    "        \n",
    "        '''\n",
    "        Here is where we define our neural network model - the Net class is inside BIOF050, so we have to call\n",
    "        it accordingly \n",
    "        \n",
    "        We use the length of our first data point to set the length of our input data (they are all the same)\n",
    "        \n",
    "        The number of class is equal to the number of unique values (the set) of our training labels\n",
    "        '''\n",
    "        neural_network = BIOF050_CNN_Final.CNet(n_channels,hidden_dimensions,kernel_size,len(set(train_labels)),pool)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Here, we use the torch.optim package to create our stochastic gradient descent function\n",
    "        \n",
    "        neural_network.parameters() reads internal information from our NN \n",
    "        (don't worry about that - SGD just requires it)\n",
    "        \n",
    "        lr is the learning rate\n",
    "        '''\n",
    "        optimizer = optim.SGD(neural_network.parameters(), lr=lr)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Here, we use the nn package to create our cross entropy loss function\n",
    "        '''\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        \n",
    "                \n",
    "        '''\n",
    "        The train function tells the neural network that it is about to be trained and that it \n",
    "        will have to calculate the needed information for optimization \n",
    "        \n",
    "        This function should always be called before training\n",
    "        '''\n",
    "        neural_network.train()\n",
    "        \n",
    "        \n",
    "        ''' This loop moves through the data once for each epoch'''\n",
    "        for i in range(n_epochs):\n",
    "            \n",
    "            ### track the number we get correct\n",
    "            correct = 0\n",
    "            \n",
    "            ''' This loop moves through each batch and feeds into the neural network'''\n",
    "            for ii in range(len(train_batches)):\n",
    "                \n",
    "                ''' \n",
    "                Clears previous gradients from the optimizer - the optimizer,\n",
    "                in this case, does not need to know what happened last time\n",
    "                '''\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                \n",
    "                batch = train_batches[ii]\n",
    "                labels = train_label_batches[ii]\n",
    "\n",
    "                \n",
    "                ''' \n",
    "                Puts our batch into the neural network after converting it to a tensor\n",
    "                \n",
    "                Pytorch wants numeric data to be floats, so we will convert to a float as well \n",
    "                using np.float32\n",
    "                \n",
    "                Predictions: For each data point in our batch, we would get something that looks like:\n",
    "                tensor([0.3,0.7]) where each number corresponds to the probability of a class\n",
    "                '''\n",
    "                predictions = neural_network(torch.tensor(np.asarray(batch).astype(np.float32)))\n",
    "                \n",
    "                \n",
    "                ''' \n",
    "                We put our probabilities into the loss function to calculate the error for this batch\n",
    "                \n",
    "                '''\n",
    "                loss = loss_function(predictions,torch.LongTensor(labels))\n",
    "                \n",
    "                '''\n",
    "                loss.backward calculates the partial derivatives that we need to optimize\n",
    "                '''\n",
    "                loss.backward()\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                optimizer step calculates the weight updates so the neural network can update the weights \n",
    "                '''\n",
    "                optimizer.step()\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                We extract just the data from our predictions, not other stuff Pytorch includes in that object\n",
    "                \n",
    "                We can then use the argmax function to figure out which index corresponds to the highest probability.\n",
    "                If it is the 0th index, and the label is zero, we add one to correct. \n",
    "                If it is the 1st index, and the label is one, we add one to correct.\n",
    "                \n",
    "                '''\n",
    "                for n,pred in enumerate(predictions.data):\n",
    "                    if labels[n] == torch.argmax(pred):\n",
    "                        correct += 1\n",
    "                        \n",
    "                        \n",
    "            print(\"Accuracy for Epoch # \" + str(i) + \": \" + str(correct/len(train_data)))\n",
    "\n",
    "        print()\n",
    "        \n",
    "\n",
    "                    \n",
    "        '''\n",
    "        The eval function tells the neural network that it is about to be tested on blind test data\n",
    "        and shouldn't change any of its internal parameters\n",
    "        \n",
    "        This function should always be called before eval\n",
    "        '''\n",
    "        neural_network.eval()\n",
    "        \n",
    "        test_correct = 0\n",
    "        \n",
    "        ''' input our test data into the neural network'''\n",
    "        predictions = neural_network(torch.tensor(np.asarray(test_data).astype(np.float32)))\n",
    "        \n",
    "        ''' checks how many we got right - very simple!'''\n",
    "        for n,pred in enumerate(predictions.data):\n",
    "            if test_labels[n] == torch.argmax(pred):\n",
    "                    test_correct += 1\n",
    "                    \n",
    "        print(\"Accuracy on test set: \" + str(test_correct/len(test_data)))\n",
    "           \n",
    "        return neural_network\n",
    "        \n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "''' Utility Function - function to turn the data into batches'''\n",
    "\n",
    "def batchify(data,labels,batch_size=16):\n",
    "    \n",
    "    batches= []\n",
    "    label_batches = []\n",
    "\n",
    "\n",
    "    for n in range(0,len(data),batch_size):\n",
    "        if n+batch_size < len(data):\n",
    "            batches.append(data[n:n+batch_size])\n",
    "            label_batches.append(labels[n:n+batch_size])\n",
    "\n",
    "    if len(data)%batch_size > 0:\n",
    "        batches.append(data[len(data)-(len(data)%batch_size):len(data)])\n",
    "        label_batches.append(labels[len(data)-(len(data)%batch_size):len(data)])\n",
    "        \n",
    "    return batches,label_batches\n",
    "\n",
    "\n",
    "def find_MLP_dim(image_size,kernel_size,n_kernels):\n",
    "    \n",
    "    after_conv1_pool = round((image_size-(kernel_size-1))/2)\n",
    "    after_conv2_pool = int(np.floor((after_conv1_pool- (kernel_size-1))/2))\n",
    "    after_conv2_pool *= after_conv2_pool\n",
    "\n",
    "    return after_conv2_pool*n_kernels\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torchvision.datasets.MNIST(\n",
    "    root = './data/MNIST',\n",
    "    download = True)\n",
    "\n",
    "\n",
    "### get the data and labels from the data object\n",
    "labels = data.targets\n",
    "data = data.data\n",
    "\n",
    "\n",
    "newdata = []\n",
    "### flatten the data, scale it from 0-1, reshape it back into 2-d form (in this case, 28x28 - varies with dataset)\n",
    "for image in data:\n",
    "   image = np.ravel(image).astype(np.float64)\n",
    "   image *= 1/image.max()\n",
    "   newdata.append(image.reshape(1,28,28))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-54e7073739ab>:96: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(batch)\n"
     ]
    }
   ],
   "source": [
    "testclass = BIOF050_CNN_Final(newdata,labels)\n",
    "model = testclass.train_test(test_size=0.2,n_epochs=3,n_channels=1,hidden_dimensions=100,batch_size=16,\n",
    "                             kernel_size=5,pool=2,lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
