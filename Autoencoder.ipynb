{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIOF050_A:\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        \n",
    "        Inside this Autoencoder class, we can define what we want out autoencoder to look like!\n",
    "        We will use nn.Sequential to keep things organized, since we have both an encoder and a\n",
    "        decoder to worry about!\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    \n",
    "    class Autoencoder(nn.Module):\n",
    "        \n",
    "        def __init__(self,data_length,hidden_dimension,bottleneck_dimension):\n",
    "    \n",
    "            super(BIOF050_A.Autoencoder, self).__init__()\n",
    "            \n",
    "            '''\n",
    "            Encoder - turns input data into low-dimensional bottlenck\n",
    "            \n",
    "            '''\n",
    "            self.encoder = nn.Sequential(\n",
    "            nn.Linear(data_length,hidden_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dimension,bottleneck_dimension))\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            Decoder: turns bottleneck layer back into original input\n",
    "            '''\n",
    "            \n",
    "            self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_dimension,hidden_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dimension,data_length),\n",
    "                \n",
    "            ### we use the Tanh function to smooth/scale the data before comparing with input\n",
    "            ### since we are comparing input with output, can't use softmax probability here - we\n",
    "            ### need actual values\n",
    "            nn.Tanh())\n",
    "    \n",
    "        '''\n",
    "        Now, all we have to do is call both the encoder and decoder in our forward method!\n",
    "        \n",
    "        ''' \n",
    "        \n",
    "        def forward(self, batch):\n",
    "            batch = self.encoder(batch)\n",
    "            batch = self.decoder(batch)\n",
    "            return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our Autoencoder\n",
    "Now, we will add in our train_test + batchify method from last time to use our Autoencoder on a dataset! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIOF050_A_Final:\n",
    "    \n",
    "    \n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        \n",
    "        Inside this Autoencoder class, we can define what we want out autoencoder to look like!\n",
    "        We will use nn.Sequential to keep things organized, since we have both an encoder and a\n",
    "        decoder to worry about!\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    \n",
    "    class Autoencoder(nn.Module):\n",
    "        \n",
    "        def __init__(self,data_length,hidden_dimension,bottleneck_dimension):\n",
    "    \n",
    "            super(BIOF050_A_Final.Autoencoder, self).__init__()\n",
    "            \n",
    "            '''\n",
    "            Encoder - turns input data into low-dimensional bottlenck\n",
    "            \n",
    "            '''\n",
    "            self.encoder = nn.Sequential(\n",
    "            nn.Linear(data_length,hidden_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dimension,bottleneck_dimension))\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            Decoder: turns bottleneck layer back into original input\n",
    "            '''\n",
    "            \n",
    "            self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_dimension,hidden_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dimension,data_length),\n",
    "                \n",
    "            ### we use the Tanh function to smooth/scale the data before comparing with input\n",
    "            ### since we are comparing input with output, can't use softmax probability here - we\n",
    "            ### need actual values\n",
    "            nn.Tanh())\n",
    "    \n",
    "        '''\n",
    "        Now, all we have to do is call both the encoder and decoder in our forward method!\n",
    "        \n",
    "        ''' \n",
    "        \n",
    "        def forward(self, batch):\n",
    "            batch = self.encoder(batch)\n",
    "            batch = self.decoder(batch)\n",
    "            return batch\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def train_test(self,hidden_dimension,bottleneck_size,batch_size,n_epochs,lr):\n",
    "            \n",
    "           \n",
    "            ''' We just batchify the whole dataset - no need for train/test here as we \n",
    "            want to reduce our entire dataset'''\n",
    "            batches = batchify_autoencoder(self.data,batch_size=batch_size)\n",
    "  \n",
    "            '''\n",
    "            For this, we use a hidden dimension of 64 for our first linear layer and a\n",
    "            bottleneck layer of 5\n",
    "            '''\n",
    "            neural_network = BIOF050_A_Final.Autoencoder(len(np.ravel(self.data[0])),hidden_dimension,bottleneck_size)\n",
    "        \n",
    "        \n",
    "            ''' Same optimizer as before'''\n",
    "            optimizer = optim.SGD(neural_network.parameters(), lr=0.01)\n",
    "        \n",
    "            ''' We use the mean squared error function here because it works well for tasks that do \n",
    "            not have discrete labels but are just compare values (like our autoencoder)'''\n",
    "            loss_function = nn.MSELoss()\n",
    "        \n",
    "            ''' Pretty much the same training process as before'''\n",
    "            neural_network.train()\n",
    "        \n",
    "            for i in range(n_epochs):\n",
    "                error = 0\n",
    "                for ii in range(len(batches)):\n",
    "                \n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                    batch = batches[ii]\n",
    "\n",
    "                    predictions = neural_network(torch.tensor(np.asarray(batch).astype(np.float32)))\n",
    "                    \n",
    "                    ### input the predicted data and the original data into the MSE loss function\n",
    "                    loss = loss_function(predictions,torch.tensor(np.asarray(batch).astype(np.float32)))\n",
    "                \n",
    "                    loss.backward()\n",
    "                \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    error += loss.data\n",
    "                    \n",
    "                print('Error: ' + str((error/len(self.data)*16)))\n",
    "\n",
    "            return neural_network\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "''' Utility Function - function to turn the data into batches'''\n",
    "\n",
    "def batchify_autoencoder(data,batch_size=16):\n",
    "    \n",
    "    batches= []\n",
    "\n",
    "\n",
    "    for n in range(0,len(data),batch_size):\n",
    "        if n+batch_size < len(data):\n",
    "            batches.append(data[n:n+batch_size])\n",
    "            \n",
    "\n",
    "    if len(data)%batch_size > 0:\n",
    "        batches.append(data[len(data)-(len(data)%batch_size):len(data)])\n",
    "\n",
    "        \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' We are using torchvision MNIST, just like our CNN - we can compare these results with the\n",
    "Autoencoder CNN (next tutorial) to see which one works better!'''\n",
    "\n",
    "data = torchvision.datasets.MNIST(\n",
    "    root = './data/MNIST',\n",
    "    download = True)\n",
    "\n",
    "\n",
    "labels = data.targets\n",
    "data = data.data\n",
    "newdata = []\n",
    "\n",
    "for image in data:\n",
    "   image = np.ravel(image).astype(np.float64)\n",
    "   image *= 1/image.max()\n",
    "   newdata.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: tensor(0.1160)\n",
      "Error: tensor(0.0975)\n",
      "Error: tensor(0.0829)\n"
     ]
    }
   ],
   "source": [
    "testclass = BIOF050_A_Final(newdata)\n",
    "model = testclass.train_test(hidden_dimension=64,bottleneck_size=5,batch_size=16,n_epochs=3,lr=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
