{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy as cp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import timeit\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIOF050_SelfSup:\n",
    "    \n",
    "    \n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "  \n",
    "    \n",
    "    '''\n",
    "        \n",
    "        Inside this Network class, we can define what we want our neural network to look like!\n",
    "        We want a network that takes some part of the data as input (input_dim)\n",
    "        and predicts some of the missing features (output_dim)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    class Network(nn.Module):\n",
    "        def __init__(self,input_dim,output_dim,hidden_dimensions):\n",
    "            super(BIOF050_SelfSup.Network, self).__init__()\n",
    "    \n",
    "            self.input_layer = nn.Linear(input_dim,hidden_dimensions)\n",
    "            self.layer1 = nn.Linear(hidden_dimensions, hidden_dimensions)\n",
    "            self.layer2 = nn.Linear(hidden_dimensions, hidden_dimensions)\n",
    "            self.output_layer = nn.Linear(hidden_dimensions,output_dim)\n",
    "            self.batch_norm = nn.BatchNorm1d(hidden_dimensions)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.tanh = nn.Tanh()\n",
    "     \n",
    "            \n",
    "            \n",
    "        def forward(self,batch): \n",
    "      \n",
    "            batch = self.input_layer(batch)\n",
    "            batch = self.batch_norm(batch)\n",
    "            batch = self.relu(batch)\n",
    "            batch = self.layer1(batch)\n",
    "            batch = self.batch_norm(batch)\n",
    "            batch = self.relu(batch)\n",
    "            batch = self.layer2(batch)\n",
    "            batch = self.batch_norm(batch)\n",
    "            batch = self.relu(batch)\n",
    "            batch = self.output_layer(batch)\n",
    "            \n",
    "            return self.tanh(batch)\n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "    def train_test(self,n_epochs,output_dim,hidden_dimensions,batch_size,lr):\n",
    "            \n",
    "        ## creating the batches using the batchify function\n",
    "        train_batches = batchify(self.data,batch_size=batch_size)\n",
    "        \n",
    "        '''\n",
    "        Here is where we define our neural network model - the Network class is inside BIOF050, so we have to call\n",
    "        it accordingly \n",
    "        \n",
    "        We use the length of our first data point to set the length of our input data (they are all the same)\n",
    "        \n",
    "        The output_dim is equal to the number of missing data points we want to predict\n",
    "        '''\n",
    "        neural_network = BIOF050_SelfSup.Network(len(train_batches[0][0]),output_dim,hidden_dimensions)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Here, we use the torch.optim package to create our stochastic gradient descent function\n",
    "        \n",
    "        neural_network.parameters() reads internal information from our NN \n",
    "        (don't worry about that - Adam just requires it)\n",
    "        \n",
    "        We will use Adam, a powerful optimization function that optimizes learning rates \n",
    "        for the neurons\n",
    "        \n",
    "        lr is the learning rate\n",
    "        '''\n",
    "        optimizer = optim.Adam(neural_network.parameters(), lr=lr)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Here, we are comparing the predicted values with the actual values for the missing data,\n",
    "        so we use MSE\n",
    "        '''\n",
    "        loss_function = nn.MSELoss()\n",
    "        \n",
    "                \n",
    "        '''\n",
    "        The train function tells the neural network that it is about to be trained and that it \n",
    "        will have to calculate the needed information for optimization \n",
    "        \n",
    "        This function should always be called before training\n",
    "        '''\n",
    "        neural_network.train()\n",
    "        \n",
    "        \n",
    "        ''' This loop moves through the data once for each epoch'''\n",
    "        for i in range(n_epochs):\n",
    "            \n",
    "            \n",
    "            total = 0\n",
    "            \n",
    "            ''' This loop moves through each batch and feeds into the neural network'''\n",
    "            for ii in range(len(train_batches)):\n",
    "                \n",
    "                ''' \n",
    "                Clears previous gradients from the optimizer - the optimizer,\n",
    "                in this case, does not need to know what happened last time\n",
    "                '''\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                \n",
    "                batch = train_batches[ii]\n",
    "  \n",
    "                inputs = []\n",
    "                targets = []\n",
    "                batch_copy = cp.deepcopy(train_batches[ii])\n",
    "   \n",
    "                ''' \n",
    "                for each vector in our batch, we remove some of the data, we save it \n",
    "                as the target, and then we set those values to zero in the original data\n",
    "                If zero, these will not be considered (zero *weight = 0)\n",
    "                \n",
    "                This is where we do our self-supervision - we are predicting missing \n",
    "                data using the rest of it!\n",
    "                \n",
    "                '''\n",
    "                for n in range(len(batch_copy)):\n",
    "                    selection = np.random.choice(list(range(len(batch_copy[n]))),output_dim,replace=False)\n",
    "                    targets.append(batch_copy[n][selection])\n",
    "                    batch_copy[n][selection] = 0\n",
    "         \n",
    "\n",
    "                \n",
    "                ''' \n",
    "                Puts our batch into the neural network after converting it to a tensor\n",
    "                \n",
    "                Pytorch wants numeric data to be floats, so we will convert to a float as well \n",
    "                using np.float32\n",
    "                \n",
    "                '''\n",
    "                predictions = neural_network(torch.tensor(np.asarray(batch_copy).astype(np.float32)))\n",
    "        \n",
    "            \n",
    "                \n",
    "                ''' \n",
    "                We put our predicted values into the loss function to calculate the error for this batch\n",
    "                \n",
    "                '''\n",
    "                loss = loss_function(predictions,torch.tensor(np.asarray(targets).astype(np.float32)))\n",
    "                \n",
    "                \n",
    "                total += loss\n",
    "                \n",
    "                '''\n",
    "                loss.backward calculates the partial derivatives that we need to optimize\n",
    "                '''\n",
    "                loss.backward()\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                optimizer step calculates the weight updates so the neural network can update the weights \n",
    "                '''\n",
    "                optimizer.step()\n",
    "                \n",
    "            print(total/len(train_batches))\n",
    "    \n",
    "           \n",
    "        return neural_network\n",
    "        \n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "''' Utility Function - function to turn the data into batches'''\n",
    "\n",
    "def batchify(data,batch_size=16):\n",
    "    \n",
    "    batches= []\n",
    "\n",
    "\n",
    "\n",
    "    for n in range(0,len(data),batch_size):\n",
    "        if n+batch_size < len(data):\n",
    "            batches.append(data[n:n+batch_size])\n",
    "            \n",
    "\n",
    "    if len(data)%batch_size > 0:\n",
    "        batches.append(data[len(data)-(len(data)%batch_size):len(data)])\n",
    "        \n",
    "    return batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MinMaxScaler().fit_transform(pd.read_csv('data.csv').drop(['score'],1).values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0044, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9cb8cd4e7d36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtestclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBIOF050_SelfSup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = testclass.train_test(n_epochs=10,output_dim=1,hidden_dimensions=100,batch_size=16,\n\u001b[0m\u001b[1;32m      3\u001b[0m                              lr=0.001)\n",
      "\u001b[0;32m<ipython-input-10-df59f6be5c64>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(self, n_epochs, output_dim, hidden_dimensions, batch_size, lr)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 '''\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-df59f6be5c64>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \"\"\"\n\u001b[0;32m--> 131\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2054\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2057\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "testclass = BIOF050_SelfSup(data)\n",
    "model = testclass.train_test(n_epochs=10,output_dim=1,hidden_dimensions=100,batch_size=16,\n",
    "                             lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
